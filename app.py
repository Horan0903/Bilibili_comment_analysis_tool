import streamlit as st
import pandas as pd
from utils.data_loader import load_data_from_csv, load_data_from_api, load_bilibili_comments
from utils.text_analysis import perform_sentiment_analysis, extract_keywords
from utils.visualization import generate_wordcloud, plot_time_trend, plot_interaction_stats
import plotly.express as px
import openai
from datetime import datetime
import os
from openai import OpenAI
import gensim
from gensim import corpora
import pyLDAvis
import pyLDAvis.gensim_models
import streamlit.components.v1 as components
import jieba
from sklearn.feature_extraction.text import CountVectorizer
from gensim.matutils import Sparse2Corpus
import io
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# è®¾ç½®OpenAI APIå¯†é’¥
openai.api_key = os.getenv("OPENAI_API_KEY")

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Bç«™è¯„è®ºåˆ†æå·¥å…·",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åŠ è½½è‡ªå®šä¹‰CSS
with open('assets/style.css') as f:
    st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# æ·»åŠ åœç”¨è¯åŠ å‡½æ•°
def load_stopwords():
    """åŠ è½½åœç”¨è¯"""
    try:
        with open('LDA/Stopword.txt', encoding='utf-8') as f:
            stopwords = set(line.strip() for line in f)
        return stopwords
    except FileNotFoundError:
        st.warning("åœç”¨è¯æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ç©ºçš„åœç”¨è¯é›†")
        return set()

# æ·»åŠ æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text, stopwords):
    """æ–‡æœ¬é¢„å¤„ç†ï¼šåˆ†è¯å¹¶å»é™¤åœç”¨è¯"""
    if not isinstance(text, str):
        text = str(text) if isinstance(text, (float, int)) else ''
    
    seg = jieba.cut(text)
    return ' '.join([i for i in seg if i not in stopwords])

# ä¿®æ”¹perform_lda_analysiså‡½æ•°
@st.cache_data(show_spinner=False)
def perform_lda_analysis(_texts, num_topics=5):
    """
    æ‰§è¡Œ LDA ä¸»é¢˜å»ºæ¨¡åˆ†æ
    
    Args:
        _texts: è¯„è®ºæ–‡æœ¬åˆ—è¡¨
        num_topics: ä¸»é¢˜æ•°é‡
    Returns:
        lda_model: LDAæ¨¡å‹
        corpus: æ–‡æ¡£-è¯é¡¹çŸ©é˜µ
        dictionary: è¯å…¸
    """
    # åŠ è½½åœç”¨è¯
    stopwords = load_stopwords()
    
    # åŠ è½½è‡ªå®šä¹‰è¯å…¸
    try:
        user_dicts = [
            "SogouLabDic.txt", "dict_baidu_utf8.txt", "dict_pangu.txt",
            "dict_sougou_utf8.txt", "dict_tencent_utf8.txt", "my_dict.txt"
        ]
        for dict_path in user_dicts:
            jieba.load_userdict(f"LDA/{dict_path}")
    except FileNotFoundError:
        st.warning("éƒ¨åˆ†è‡ªå®šä¹‰è¯å…¸æœªæ‰¾åˆ°")
    
    # æ–‡æœ¬é¢„å¤„ç†
    processed_texts = [preprocess_text(text, stopwords) for text in _texts]
    
    # ä½¿ç”¨CountVectorizeræ„å»ºæ–‡æ¡£-è¯é¡¹çŸ©é˜µ
    vectorizer = CountVectorizer(max_df=0.95, min_df=2)
    doc_term_matrix = vectorizer.fit_transform(processed_texts)
    
    # è½¬æ¢ä¸ºgensimæ ¼å¼
    corpus = Sparse2Corpus(doc_term_matrix, documents_columns=False)
    dictionary = corpora.Dictionary.from_corpus(
        corpus, 
        id2word=dict(enumerate(vectorizer.get_feature_names_out()))
    )
    
    # è®­ç»ƒLDAæ¨¡å‹
    lda_model = gensim.models.ldamodel.LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        random_state=42,
        update_every=1,
        passes=10,
        alpha='auto',
        per_word_topics=True
    )
    
    return lda_model, corpus, dictionary

# ä¿®æ”¹è¯äº‘ç”Ÿæˆå‡½æ•°çš„ç¼“å­˜è£…é¥°å™¨
@st.cache_data(show_spinner=False)
def generate_topic_wordcloud(_lda_model, _dictionary):
    """ä¸ºæ¯ä¸ªä¸»é¢˜ç”Ÿæˆè¯äº‘"""
    wordclouds = []
    for idx, topic in enumerate(_lda_model.get_topics()):
        word_freq = dict(zip(_dictionary.values(), topic))
        wordcloud = WordCloud(
            width=800, 
            height=400, 
            max_words=50,
            font_path='LDA/Songti.ttc'  # ç¡®ä¿å­—ä½“æ–‡ä»¶å­˜åœ¨
        ).generate_from_frequencies(word_freq)
        
        # å°†è¯äº‘å›¾ä¿å­˜åˆ°å†…å­˜
        buf = io.BytesIO()
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'ä¸»é¢˜ {idx + 1}')
        plt.savefig(buf, format="png", bbox_inches='tight')
        plt.close()
        buf.seek(0)
        wordclouds.append(buf)
    return wordclouds

def main():
    # ä¾§è¾¹æ 
    st.sidebar.title("Bç«™è¯„è®ºåˆ†æå·¥å…·")
    
    # æ•°æ®è¾“å…¥é€‰æ‹©
    data_source = st.sidebar.radio(
        "é€‰æ‹©æ•°æ®æ¥æº",
        ("æœ¬åœ°CSVæ–‡ä»¶", "Bç«™API")
    )
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    if data_source == "æœ¬åœ°CSVæ–‡ä»¶":
        uploaded_file = st.sidebar.file_uploader(
            "ä¸Šä¼ æ•°æ®æ–‡ä»¶", 
            type=['csv', 'xlsx', 'xls'],
            help="æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šCSVã€Excel (xlsx/xls)"
        )
        if uploaded_file is not None:
            try:
                with st.spinner('æ­£åœ¨åŠ è½½æ•°æ®...'):
                    df = load_data_from_csv(uploaded_file)
                
                if df is not None:  # åªæœ‰åœ¨ç”¨æˆ·ç¡®è®¤åˆ—æ˜ å°„åæ‰ç»§ç»­
                    if len(df) > 0:
                        st.success(f'æˆåŠŸåŠ è½½ {len(df)} æ¡è¯„è®ºæ•°æ®')
                        
                        # æ·»åŠ æ•°æ®é¢„è§ˆå’ŒåŸºæœ¬ç»Ÿè®¡
                        with st.expander("æ•°æ®é¢„è§ˆä¸åŸºæœ¬ç»Ÿè®¡"):
                            col1, col2 = st.columns([2, 1])
                            with col1:
                                st.dataframe(
                                    df.head(),
                                    use_container_width=True
                                )
                            with col2:
                                st.write("åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼š")
                                st.write(f"- è¯„è®ºæ€»æ•°ï¼š{len(df):,}")
                                st.write(f"- è¯„è®ºæ—¶é—´èŒƒå›´ï¼š")
                                st.write(f"  - å¼€å§‹ï¼š{df['æ—¶é—´'].min().strftime('%Y-%m-%d')}")
                                st.write(f"  - ç»“æŸï¼š{df['æ—¶é—´'].max().strftime('%Y-%m-%d')}")
                                st.write(f"- æ€»ç‚¹èµæ•°ï¼š{df['ç‚¹èµæ•°'].sum():,}")
                                st.write(f"- æ€»å›å¤æ•°ï¼š{df['å›å¤æ•°'].sum():,}")
                        
                        # æ·»åŠ æ•°æ®è¿‡æ»¤é€‰é¡¹
                        with st.expander("æ•°æ®è¿‡æ»¤é€‰é¡¹"):
                            col1, col2 = st.columns(2)
                            with col1:
                                min_date = st.date_input(
                                    "å¼€å§‹æ—¥æœŸ",
                                    value=df['æ—¶é—´'].min().date(),
                                    min_value=df['æ—¶é—´'].min().date(),
                                    max_value=df['æ—¶é—´'].max().date()
                                )
                            with col2:
                                max_date = st.date_input(
                                    "ç»“æŸæ—¥æœŸ",
                                    value=df['æ—¶é—´'].max().date(),
                                    min_value=df['æ—¶é—´'].min().date(),
                                    max_value=df['æ—¶é—´'].max().date()
                                )
                            
                            # æ ¹æ®æ—¥æœŸè¿‡æ»¤æ•°æ®
                            mask = (df['æ—¶é—´'].dt.date >= min_date) & (df['æ—¶é—´'].dt.date <= max_date)
                            filtered_df = df.loc[mask]
                            
                            if len(filtered_df) != len(df):
                                st.info(f"å·²è¿‡æ»¤ï¼Œå½“å‰æ˜¾ç¤º {len(filtered_df)} æ¡è¯„è®ºï¼ˆå…± {len(df)} æ¡ï¼‰")
                        
                        # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®è¿›è¡Œåˆ†æ
                        display_analysis(filtered_df)
                        
                        # æ·»åŠ æ•°æ®å¯¼å‡ºé€‰é¡¹
                        with st.expander("å¯¼å‡ºåˆ†æç»“æœ"):
                            col1, col2 = st.columns(2)
                            with col1:
                                # å¯¼å‡ºåŸå§‹æ•°æ®
                                csv = filtered_df.to_csv(index=False).encode('utf-8-sig')
                                st.download_button(
                                    "ä¸‹è½½è¿‡æ»¤åçš„æ•°æ®(CSV)",
                                    csv,
                                    "bilibili_comments_filtered.csv",
                                    "text/csv",
                                    key='download-csv'
                                )
                            with col2:
                                # å¯¼å‡ºåˆ†æç»“æœ
                                analysis_results = {
                                    'è¯„è®ºæ€»æ•°': len(filtered_df),
                                    'æ—¶é—´èŒƒå›´': f"{min_date} è‡³ {max_date}",
                                    'å¹³å‡ç‚¹èµæ•°': filtered_df['ç‚¹èµæ•°'].mean(),
                                    'å¹³å‡å›å¤æ•°': filtered_df['å›å¤æ•°'].mean(),
                                    # å¯ä»¥æ·»åŠ æ›´å¤šåˆ†æç»“æœ
                                }
                                st.json(analysis_results)
                                
                    else:
                        st.warning('æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯„è®ºæ•°æ®')
            except Exception as e:
                st.error(f'åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}')
    else:
        video_id = st.sidebar.text_input(
            "è¾“å…¥è§†é¢‘BVå·æˆ–é“¾æ¥",
            help="æ”¯æŒBVå·æˆ–å®Œæ•´è§†é¢‘é“¾æ¥"
        )
        
        max_comments = st.sidebar.number_input(
            "æœ€å¤§è¯„è®ºè·å–æ•°é‡",
            min_value=10,
            max_value=10000,
            value=1000,
            step=100,
            help="è®¾ç½®ä¸ºè·å–çš„æœ€å¤§è¯„è®ºæ•°é‡ï¼ŒåŒ…æ‹¬å›å¤"
        )
        
        if video_id:
            try:
                with st.spinner('æ­£åœ¨ä»Bç«™è·å–è¯„è®ºæ•°æ®...'):
                    df = load_bilibili_comments(video_id, max_comments)
                    
                if df is not None and len(df) > 0:
                    st.success(f'æˆåŠŸè·å– {len(df)} æ¡è¯„è®ºæ•°æ®')
                    
                    # æ·»åŠ æ•°é¢„è§ˆ
                    with st.expander("æ•°æ®é¢„è§ˆ"):
                        st.dataframe(df.head(), use_container_width=True)
                        
                        # æ˜¾ç¤ºé¢å¤–çš„ç”¨æˆ·ä¿¡æ¯ç»Ÿè®¡
                        if 'ç”¨æˆ·ç­‰çº§' in df.columns:
                            st.subheader("ç”¨æˆ·ä¿¡æ¯ç»Ÿè®¡")
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                avg_level = df['ç”¨æˆ·ç­‰çº§'].mean()
                                st.metric("å¹³å‡ç”¨æˆ·ç­‰çº§", f"{avg_level:.1f}")
                                
                            with col2:
                                vip_count = (df['ä¼šå‘˜ç±»å‹'] > 0).sum()
                                st.metric("å¤§ä¼šå‘˜æ•°é‡", vip_count)
                                
                            with col3:
                                gender_dist = df['æ€§åˆ«'].value_counts()
                                st.write("æ€§åˆ«åˆ†å¸ƒï¼š")
                                st.write(gender_dist)
                    
                    # ç»§ç»­æ˜¾ç¤ºåˆ†æç»“æœ
                    display_analysis(df)
                else:
                    st.warning('æœªæ‰¾åˆ°è¯„è®ºæ•°æ®ï¼Œè¯·æ£€æŸ¥è§†é¢‘BVå·æ˜¯å¦æ­£ç¡®')
            except Exception as e:
                st.error(f'è·å–æ•°æ®æ—¶å‡ºé”™: {str(e)}')
                st.error('è¯·ç¡®ä¿è¾“å…¥çš„BVå·æ­£ç¡®ï¼Œå¹¶ä¸”è§†é¢‘å­˜åœ¨ä¸”å¯è®¿é—®')

    # æ·»åŠ é¡µè„š
    st.sidebar.markdown('---')
    st.sidebar.markdown('### ç”¨è¯´æ˜')
    st.sidebar.markdown('''
    1. é€‰æ‹©æ®æ¥æºï¼ˆæœ¬åœ°CSVæˆ–Bç«™APIï¼‰
    2. ä¸Šä¼ æ–‡ä»¶æˆ–è¾“å…¥BVå·
    3. ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ
    4. åœ¨ä¸åŒæ ‡ç­¾é¡µæŸ¥çœ‹åˆ†æç»“æœ
    ''')

def display_analysis(df):
    # æ·»åŠ é¡µé¢æ ‡é¢˜å’Œæè¿°
    st.title("Bç«™è¯„è®ºæ•°æ®åˆ†æ")
    st.markdown("""
    <div style='background-color: #f8f9fa; padding: 1rem; border-radius: 0.5rem; margin-bottom: 2rem;'>
        ğŸ“Š å®æ—¶åˆ†æè§†é¢‘è¯„è®ºæ•°æ®ï¼Œæ·±å…¥äº†è§£ç”¨æˆ·åé¦ˆå’Œäº’åŠ¨æƒ…å†µã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # åˆ›å»ºå¤šä¸ªæ ‡ç­¾é¡µï¼Œæ·»åŠ AIåˆ†ææ ‡ç­¾
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š è¯„è®ºæ¦‚è§ˆ", "ğŸ˜Š æƒ…æ„Ÿåˆ†æ", "ğŸ” å…³é”®è¯åˆ†æ", "ğŸ‘¥ äº’åŠ¨æ•°æ®", "ğŸ¤– AIåˆ†æ"
    ])
    
    with tab1:
        st.markdown("""
        <div class='stCardContainer'>
            <h2>è¯„è®ºæ¦‚è§ˆ</h2>
        </div>
        """, unsafe_allow_html=True)
        
        # æ¦‚è§ˆæŒ‡æ ‡
        metrics_container = st.container()
        with metrics_container:
            m1, m2, m3, m4 = st.columns(4)
            with m1:
                st.markdown("""
                <div class='metric-card'>
                    <div class='metric-value'>{:,}</div>
                    <div class='metric-label'>æ€»è¯„è®ºæ•°</div>
                </div>
                """.format(len(df)), unsafe_allow_html=True)
            with m2:
                st.markdown("""
                <div class='metric-card'>
                    <div class='metric-value'>{:,}</div>
                    <div class='metric-label'>æ€»ç‚¹èµæ•°</div>
                </div>
                """.format(df['ç‚¹èµæ•°'].sum()), unsafe_allow_html=True)
            with m3:
                st.markdown("""
                <div class='metric-card'>
                    <div class='metric-value'>{:,}</div>
                    <div class='metric-label'>æ€»å›å¤æ•°</div>
                </div>
                """.format(df['å›å¤æ•°'].sum()), unsafe_allow_html=True)
            with m4:
                avg_likes = df['ç‚¹èµæ•°'].mean()
                st.markdown("""
                <div class='metric-card'>
                    <div class='metric-value'>{:.1f}</div>
                    <div class='metric-label'>å¹³å‡ç‚¹èµæ•°</div>
                </div>
                """.format(avg_likes), unsafe_allow_html=True)
        
        # æ—¶é—´è¶‹åŠ¿å’Œè¯äº‘
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("<div class='chart-container'>", unsafe_allow_html=True)
            st.plotly_chart(plot_time_trend(df), use_container_width=True)
            st.markdown("</div>", unsafe_allow_html=True)
        
        with col2:
            st.markdown("<div class='chart-container'>", unsafe_allow_html=True)
            st.subheader("è¯äº‘å±•ç¤º")
            wordcloud_img = generate_wordcloud(df)
            st.image(wordcloud_img)
            st.markdown("</div>", unsafe_allow_html=True)
    
    with tab2:
        st.header("æƒ…æ„Ÿåˆ†æ")
        
        # ç®—æƒ…æ„Ÿåˆ†æç»“æœ
        with st.spinner('æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...'):
            sentiment_analysis = perform_sentiment_analysis(df)
            df['æƒ…æ„Ÿå¾—åˆ†'] = sentiment_analysis['sentiment_scores']
        
        # å®šä¹‰æƒ…æ„Ÿåˆ†ç±»
        df['æƒ…æ„Ÿç±»åˆ«'] = pd.cut(
            df['æƒ…æ„Ÿå¾—åˆ†'], 
            bins=[-0.1, 0.3, 0.7, 1.1], 
            labels=['æ¶ˆæ', 'ä¸­æ€§', 'ç§¯æ']
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            # æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
            sentiment_dist = df['æƒ…æ„Ÿç±»åˆ«'].value_counts()
            fig_pie = px.pie(
                values=sentiment_dist.values,
                names=sentiment_dist.index,
                title="è¯„è®ºæƒ…æ„Ÿåˆ†å¸ƒ",
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # æƒ…æ„Ÿå¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
            fig_hist = px.histogram(
                df,
                x='æƒ…æ„Ÿå¾—åˆ†',
                nbins=30,
                title="æƒ…æ„Ÿå¾—åˆ†åˆ†å¸ƒ",
                color_discrete_sequence=['#2ecc71']
            )
            st.plotly_chart(fig_hist, use_container_width=True)
        
        # å±•ç¤ºå…¸å‹è¯„è®º
        st.subheader("å…¸å‹è¯„è®ºç¤ºä¾‹")
        
        sentiment_tabs = st.tabs(["ç§¯æè¯„è®º", "ä¸­æ€§è¯„è®º", "æ¶ˆæè¯„è®º"])
        
        with sentiment_tabs[0]:
            positive_comments = df[df['æƒ…æ„Ÿç±»åˆ«'] == 'ç§¯æ'].nlargest(5, 'ç‚¹èµæ•°')
            if not positive_comments.empty:
                for _, comment in positive_comments.iterrows():
                    st.info(f"ğŸ‘¤ {comment['ç”¨æˆ·å']}: {comment['è¯„è®ºå†…å®¹']}\n\n"
                           f"ğŸ‘ {comment['ç‚¹èµæ•°']} | ğŸ’¬ {comment['å›å¤æ•°']} | "
                           f"ğŸ˜Š æƒ…æ„Ÿå¾—åˆ†: {comment['æƒ…æ„Ÿå¾—åˆ†']:.2f}")
            else:
                st.write("æš‚æ— ç§¯æè¯„è®º")
        
        with sentiment_tabs[1]:
            neutral_comments = df[df['æƒ…æ„Ÿç±»åˆ«'] == 'ä¸­æ€§'].nlargest(5, 'ç‚¹èµæ•°')
            if not neutral_comments.empty:
                for _, comment in neutral_comments.iterrows():
                    st.info(f"ğŸ‘¤ {comment['ç”¨æˆ·å']}: {comment['è¯„è®ºå†…å®¹']}\n\n"
                           f"ğŸ‘ {comment['ç‚¹èµæ•°']} | ğŸ’¬ {comment['å›å¤æ•°']} | "
                           f"ğŸ˜ æƒ…æ„Ÿå¾—åˆ†: {comment['æƒ…æ„Ÿå¾—åˆ†']:.2f}")
            else:
                st.write("æš‚æ— ä¸­æ€§è¯„è®º")
        
        with sentiment_tabs[2]:
            negative_comments = df[df['æƒ…æ„Ÿç±»åˆ«'] == 'æ¶ˆæ'].nlargest(5, 'ç‚¹èµæ•°')
            if not negative_comments.empty:
                for _, comment in negative_comments.iterrows():
                    st.info(f"ğŸ‘¤ {comment['ç”¨æˆ·å']}: {comment['è¯„è®ºå†…å®¹']}\n\n"
                           f"ğŸ‘ {comment['ç‚¹èµæ•°']} | ğŸ’¬ {comment['å›å¤æ•°']} | "
                           f"ğŸ˜” æƒ…æ„Ÿå¾—åˆ†: {comment['æƒ…æ„Ÿå¾—åˆ†']:.2f}")
            else:
                st.write("æš‚æ— æ¶ˆæè¯„è®º")
        
        # æ·»åŠ æƒ…æ„Ÿåˆ†æç»Ÿè®¡ä¿¡æ¯
        st.subheader("æƒ…æ„Ÿåˆ†æç»Ÿè®¡")
        stats_col1, stats_col2, stats_col3 = st.columns(3)
        
        with stats_col1:
            avg_sentiment = df['æƒ…æ„Ÿå¾—åˆ†'].mean()
            st.metric(
                "å¹³å‡æƒ…æ„Ÿå¾—åˆ†",
                f"{avg_sentiment:.2f}",
                delta=None,
                help="å¾—åˆ†èŒƒå›´0-1ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šç§¯æ"
            )
            
        with stats_col2:
            positive_rate = (df['æƒ…æ„Ÿç±»åˆ«'] == 'ç§¯æ').mean() * 100
            st.metric(
                "ç§¯æè¯„è®ºå æ¯”",
                f"{positive_rate:.1f}%",
                delta=None
            )
            
        with stats_col3:
            negative_rate = (df['æƒ…æ„Ÿç±»åˆ«'] == 'æ¶ˆæ').mean() * 100
            st.metric(
                "æ¶ˆæè¯„è®ºå æ¯”",
                f"{negative_rate:.1f}%",
                delta=None
            )
    
    with tab3:
        st.header("å…³é”®è¯åˆ†æ")
        
        # è·å–å…³é”®è¯
        keywords = extract_keywords(df)
        
        # è½¬æ¢ä¸ºDataFrameä»¥ä¾¿ç»˜å›¾
        keywords_df = pd.DataFrame(keywords, columns=['å…³é”®è¯', 'å‡ºç°æ¬¡æ•°'])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # å…³é”®è¯æŸ±çŠ¶å›¾
            fig_bar = px.bar(
                keywords_df.head(15),
                x='å…³é”®è¯',
                y='å‡ºç°æ¬¡æ•°',
                title="çƒ­é—¨å…³é”®è¯TOP15",
                color='å‡ºç°æ¬¡æ•°',
                color_continuous_scale='Viridis'
            )
            st.plotly_chart(fig_bar, use_container_width=True)
        
        with col2:
            # å…³é”®è¯è¡¨æ ¼
            st.dataframe(
                keywords_df.head(20).style.background_gradient(cmap='YlOrRd'),
                use_container_width=True
            )
        
        # æ·»åŠ LDAä¸»é¢˜å»ºæ¨¡éƒ¨åˆ†
        st.subheader("LDAä¸»é¢˜å»ºæ¨¡åˆ†æ")
        
        # å‡†å¤‡æ•°æ®
        column_name = "è¯„è®ºå†…å®¹"  # ä½¿ç”¨æ­£ç¡®çš„åˆ—å
        if column_name not in df.columns:
            st.error(f"æ‰¾ä¸åˆ°åˆ— '{column_name}'ï¼Œè¯·ç¡®ä¿æ•°æ®åŒ…å«è¯„è®ºå†…å®¹åˆ—")
            return
            
        texts = df[column_name].tolist()
        
        # æ·»åŠ ä¸»é¢˜æ•°é‡é€‰æ‹©å™¨
        num_topics = st.slider("é€‰æ‹©ä¸»é¢˜æ•°é‡", min_value=2, max_value=10, value=5)
        
        if st.button("å¼€å§‹LDAåˆ†æ"):
            with st.spinner("æ­£åœ¨è¿›è¡ŒLDAä¸»é¢˜å»ºæ¨¡åˆ†æ..."):
                # æ‰§è¡ŒLDAåˆ†æ
                lda_model, corpus, dictionary = perform_lda_analysis(texts, num_topics)
                
                # æ˜¾ç¤ºä¸»é¢˜è¯äº‘
                st.write("### ä¸»é¢˜è¯äº‘")
                wordclouds = generate_topic_wordcloud(lda_model, dictionary)
                for buf in wordclouds:
                    st.image(buf, use_container_width=True)
                
                # æ˜¾ç¤ºä¸»é¢˜è¯åˆ†å¸ƒ
                st.write("### ä¸»é¢˜è¯åˆ†å¸ƒ")
                cols = st.columns(num_topics)  # åˆ›å»ºä¸ä¸»é¢˜æ•°é‡ç›¸åŒçš„åˆ—
                for idx, (topic_num, topic) in enumerate(lda_model.print_topics(-1)):
                    with cols[idx]:  # åœ¨æ¯ä¸ªåˆ—ä¸­æ˜¾ç¤ºä¸€ä¸ªä¸»é¢˜
                        st.write(f'ä¸»é¢˜ {idx + 1}:')
                        words = [(word.split('*')[1].strip().replace('"', ''), 
                                 float(word.split('*')[0])) 
                                for word in topic.split(' + ')]
                        topic_df = pd.DataFrame(words, columns=['è¯è¯­', 'æƒé‡'])
                        st.dataframe(topic_df)
                
                # ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–
                vis_data = pyLDAvis.gensim_models.prepare(
                    lda_model, corpus, dictionary, mds='mmds')
                
                # ç¿»è¯‘å¹¶æ˜¾ç¤ºHTML
                html_string = pyLDAvis.prepared_data_to_html(vis_data)
                translations = {
                    "Topic": "ä¸»é¢˜",
                    "Lambda": "Î» å€¼",
                    "Relevance": "å…³è”åº¦",
                    "Overall term frequency": "æ•´ä½“è¯é¢‘",
                    "Top 30 Most Salient Terms": "å‰30ä¸ªæœ€æ˜¾è‘—çš„è¯è¯­",
                    "Most relevant words for topic": "ä¸»é¢˜çš„ç›¸å…³è¯è¯­",
                }
                for english, chinese in translations.items():
                    html_string = html_string.replace(english, chinese)
                
                components.html(html_string, width=1300, height=800)
                
                # ä¸»é¢˜åˆ†å¸ƒçƒ­åŠ›å›¾
                doc_topics = []
                for doc in corpus:
                    topic_probs = lda_model.get_document_topics(doc)
                    probs = [0] * num_topics
                    for topic_id, prob in topic_probs:
                        probs[topic_id] = prob
                    doc_topics.append(probs)
                
                topic_dist_df = pd.DataFrame(doc_topics)
                topic_dist_df.columns = [f'ä¸»é¢˜{i+1}' for i in range(num_topics)]
                
                fig = px.imshow(
                    topic_dist_df.T,
                    labels=dict(x="æ–‡æ¡£", y="ä¸»é¢˜", color="æ¦‚ç‡"),
                    title="æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçƒ­åŠ›å›¾"
                )
                st.plotly_chart(fig)
        
        # å…³é”®è¯æœç´¢åŠŸèƒ½
        st.subheader("å…³é”®è¯æœç´¢")
        search_word = st.text_input("è¾“å…¥å…³é”®è¯æœç´¢ç›¸å…³è¯„è®ºï¼š")
        if search_word:
            filtered_comments = df[df['è¯„è®ºå†…å®¹'].str.contains(search_word, na=False)]
            if not filtered_comments.empty:
                st.write(f"æ‰¾åˆ° {len(filtered_comments)} æ¡ç›¸å…³è¯„è®ºï¼š")
                for _, comment in filtered_comments.iterrows():
                    st.info(f"ğŸ‘¤ {comment['ç”¨æˆ·å']}: {comment['è¯„è®ºå†…å®¹']}\n\n"
                           f"ğŸ‘ {comment['ç‚¹èµæ•°']} | ğŸ’¬ {comment['å›å¤æ•°']}")
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³è¯„è®º")
    
    with tab4:
        st.header("äº’åŠ¨æ•°æ®")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ç‚¹èµæ•°åˆ†å¸ƒ
            fig_likes = px.histogram(
                df,
                x='ç‚¹èµæ•°',
                title="è¯„è®ºç‚¹èµæ•°åˆ†å¸ƒ",
                color_discrete_sequence=['#3498db']
            )
            st.plotly_chart(fig_likes, use_container_width=True)
        
        with col2:
            # å›å¤æ•°åˆ†å¸ƒ
            fig_replies = px.histogram(
                df,
                x='å›å¤æ•°',
                title="è¯„è®ºå›å¤æ•°åˆ†å¸ƒ",
                color_discrete_sequence=['#e74c3c']
            )
            st.plotly_chart(fig_replies, use_container_width=True)
        
        # äº’åŠ¨æœ€é«˜çš„è¯„è®º
        st.subheader("äº’åŠ¨æœ€é«˜çš„è¯„è®º")
        df['äº’åŠ¨æ€»æ•°'] = df['ç‚¹èµæ•°'] + df['å›å¤æ•°']
        top_comments = df.nlargest(5, 'äº’åŠ¨æ€»æ•°')
        for _, comment in top_comments.iterrows():
            st.success(f"ğŸ‘¤ {comment['ç”¨æˆ·å']}: {comment['è¯„è®ºå†…å®¹']}\n\n"
                      f"ğŸ‘ {comment['ç‚¹èµæ•°']} | ğŸ’¬ {comment['å›å¤æ•°']}")
    
    with tab5:
        st.header("AI è¯„è®ºåˆ†æ")
        
        # APIé…ç½®
        api_key = st.text_input("API å¯†é’¥", type="password")
        base_url = st.text_input("Base URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
        model_name = st.text_input("æ¨¡å‹åç§°", "qwen-turbo")
        
        # ç³»ç»Ÿæç¤ºè¯­
        system_prompt = st.text_area(
            "ç³»ç»Ÿæç¤ºè¯­", 
            "You are a helpful assistant that analyzes video comments. Please analyze if each comment is related to the given keywords. Answer with only 'æ˜¯' or 'å¦'."
        )
        
        # å…³é”®è¯è¾“å…¥
        keywords = st.text_input(
            "è¾“å…¥å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰",
            "æ•°æ®åˆ†æ,æ•°æ®,å¯è§†åŒ–,å…«çˆªé±¼,æ•°æ®å»ºæ¨¡",
            help="è¾“å…¥ä½ æƒ³åˆ†æçš„å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”"
        )
        
        # æ„å»ºæç¤ºè¯æ¨¡æ¿
        user_prompt_template = (
            "è¯·åˆ†æä»¥ä¸‹è¯„è®ºæ˜¯å¦ä¸è¿™äº›å…³é”®è¯ç›¸å…³ï¼š{keywords}\n"
            "åªéœ€å›ç­”'æ˜¯'æˆ–'å¦'ã€‚è¯„è®ºå†…å®¹ï¼š{comment}"
        )
        
        # å‚æ•°è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            temperature = st.slider("Temperature", 0.0, 1.0, 0.8)
        with col2:
            top_p = st.slider("Top P", 0.0, 1.0, 0.8)
        
        # é€‰æ‹©è¦åˆ†æçš„è¯„è®ºæ•°é‡
        num_comments = st.number_input(
            "åˆ†æè¯„è®ºæ•°é‡", 
            min_value=1,
            max_value=min(len(df), 100),
            value=min(10, len(df))
        )
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½å·²å¡«å†™
        if st.button("å¼€å§‹AIåˆ†æ"):
            if not api_key:
                st.error("è¯·æä¾› API å¯†é’¥")
            elif not base_url:
                st.error("è¯·æä¾› Base URL")
            elif not model_name:
                st.error("è¯·æä¾›æ¨¡å‹åç§°")
            else:
                try:
                    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
                    client = OpenAI(
                        api_key=api_key,
                        base_url=base_url
                    )
                    
                    # è·å–å¾…åˆ†æçš„è¯„è®º
                    top_comments = df.nlargest(num_comments, 'ç‚¹èµæ•°')
                    
                    # åˆ›å»ºè¿›åº¦æ¡
                    progress_bar = st.progress(0)
                    log_window = st.empty()
                    
                    # å­˜å‚¨åˆ†ç±»ç»“æœ
                    classifications = []
                    
                    # å¤„ç†å…³é”®è¯
                    keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
                    keywords_str = 'ã€'.join(keywords_list)
                    
                    for i, (_, comment) in enumerate(top_comments.iterrows()):
                        log_window.text(f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{num_comments} æ¡è¯„è®º...")
                        
                        try:
                            # è°ƒç”¨APIè¿›è¡Œåˆ†æ
                            completion = client.chat.completions.create(
                                model=model_name,
                                messages=[
                                    {'role': 'system', 'content': system_prompt},
                                    {'role': 'user', 'content': user_prompt_template.format(
                                        keywords=keywords_str,
                                        comment=comment['è¯„è®ºå†…å®¹']
                                    )}
                                ],
                                temperature=temperature,
                                top_p=top_p
                            )
                            
                            classification = completion.choices[0].message.content.strip()
                            classifications.append(classification)
                            
                        except Exception as e:
                            st.error(f"åˆ†æè¯„è®ºæ—¶å‡ºé”™: {str(e)}")
                            classifications.append("æ— æ³•åˆ†ç±»")
                        
                        # æ›´æ–°è¿›åº¦
                        progress_bar.progress((i + 1) / num_comments)
                    
                    # æ˜¾ç¤ºåˆ†æç»“æœ
                    st.success(f"åˆ†æå®Œæˆï¼å…³é”®è¯ï¼š{keywords_str}")
                    
                    # å°†åˆ†ç±»ç»“æœæ·»åŠ åˆ°DataFrame
                    results_df = top_comments.copy()
                    results_df['åˆ†ç±»ç»“æœ'] = classifications
                    
                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    related_comments = results_df[results_df['åˆ†ç±»ç»“æœ'] == 'æ˜¯']
                    total_likes = results_df['ç‚¹èµæ•°'].sum()
                    related_likes = related_comments['ç‚¹èµæ•°'].sum()
                    
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    st.subheader("åˆ†æç»Ÿè®¡")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("åˆ†æè¯„è®ºæ€»æ•°", f"{len(results_df)}")
                    with col2:
                        st.metric("ç›¸å…³è¯„è®ºæ•°", f"{len(related_comments)}")
                    with col3:
                        weighted_ratio = related_likes / total_likes if total_likes > 0 else 0
                        st.metric("ç›¸å…³è¯„è®ºåŠ æƒå æ¯”", f"{weighted_ratio:.2%}")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                    st.subheader("è¯¦ç»†åˆ†æç»“æœ")
                    st.dataframe(
                        results_df[['ç”¨æˆ·å', 'è¯„è®ºå†…å®¹', 'ç‚¹èµæ•°', 'åˆ†ç±»ç»“æœ']],
                        use_container_width=True
                    )
                    
                    # æä¾›ä¸‹è½½é€‰é¡¹
                    csv = results_df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        "ä¸‹è½½åˆ†æç»“æœ",
                        csv,
                        "comment_analysis_results.csv",
                        "text/csv",
                        key='download-analysis'
                    )
                    
                except Exception as e:
                    st.error(f"AIåˆ†æå‡ºé”™: {str(e)}")

if __name__ == "__main__":
    main() 